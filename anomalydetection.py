# -*- coding: utf-8 -*-
"""anomalyDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QFR_RJg6Jv-ctG5RW_PTfIiZVATOpdNW
"""

# === KAGGLE DATASET IMPORT WITH MEMORY TRACKING ===

# Install required package
!pip install kagglehub[pandas-datasets] -q

import kagglehub
import pandas as pd
import os
import psutil
import gc

print("ðŸ”„ Loading dataset from Kaggle...")

# Download the entire dataset
dataset_path = kagglehub.dataset_download("computingvictor/transactions-fraud-datasets")
print(f"ðŸ“ Dataset downloaded to: {dataset_path}")

# List all files in the dataset
print("\nðŸ“„ Files in dataset:")
files = os.listdir(dataset_path)
for file in files:
    print(f"   - {file}")

# Load each file with memory tracking
print("\nðŸ“Š Loading files with memory usage:")

def get_memory_usage():
    """Get current memory usage in MB"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

# Track memory before loading
initial_memory = get_memory_usage()
print(f"\nðŸ’¾ Initial memory usage: {initial_memory:.2f} MB")

# Dictionary to store all dataframes
dataframes = {}

# Load each CSV/JSON file
for file in files:
    file_path = os.path.join(dataset_path, file)
    memory_before = get_memory_usage()

    try:
        if file.endswith('.csv'):
            df_name = file.replace('.csv', '').replace('_data', '').replace('_', ' ').title()
            dataframes[df_name] = pd.read_csv(file_path)
            print(f"âœ… {df_name}: {dataframes[df_name].shape}")

        elif file.endswith('.json'):
            df_name = file.replace('.json', '').replace('_', ' ').title()
            if file == 'train_fraud_labels.json':
                import json
                with open(file_path, 'r') as f:
                    json_data = json.load(f)
                # Convert to dataframe like your original code
                transaction_labels_dict = json_data['target']
                dataframes['Fraud Labels'] = pd.Series(transaction_labels_dict).reset_index()
                dataframes['Fraud Labels'].columns = ['transaction_id', 'is_fraud']
                dataframes['Fraud Labels']['transaction_id'] = pd.to_numeric(dataframes['Fraud Labels']['transaction_id'])
            elif file == 'mcc_codes.json':
                import json
                with open(file_path, 'r') as f:
                    mcc_data = json.load(f)
                dataframes['MCC Codes'] = pd.Series(mcc_data).reset_index()
                dataframes['MCC Codes'].columns = ['mcc_code', 'description']
            print(f"âœ… {df_name}")

        memory_after = get_memory_usage()
        memory_used = memory_after - memory_before
        print(f"   ðŸ“ˆ Memory used: +{memory_used:.2f} MB")

    except Exception as e:
        print(f"âŒ Error loading {file}: {e}")

# Final memory usage
final_memory = get_memory_usage()
total_memory_used = final_memory - initial_memory

print(f"\nðŸŽ¯ LOADING COMPLETE!")
print(f"ðŸ’¾ Total memory used: {total_memory_used:.2f} MB")
print(f"ðŸ’¾ Final memory usage: {final_memory:.2f} MB")

# Display dataset information
print(f"\nðŸ“¦ DATASETS LOADED:")
print("=" * 50)
for name, df in dataframes.items():
    print(f"ðŸ“Š {name}:")
    print(f"   Shape: {df.shape}")
    print(f"   Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    print(f"   Columns: {list(df.columns)}")
    print(f"   First 3 rows:")
    print(f"   {df.head(3).to_string().replace(chr(10), chr(10) + '   ')}")
    print()

# Show available dataframes in current session
print("ðŸŽ¯ Available dataframes in your session:")
for name in dataframes.keys():
    print(f"   - {name}")

# Show system memory info
print(f"\nðŸ’» System Memory Info:")
print(f"   Available RAM: {psutil.virtual_memory().available / 1024**3:.1f} GB")
print(f"   Total RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB")
print(f"   Memory usage: {psutil.virtual_memory().percent}%")

print(f"\nâœ… Ready to use! Access your data with: dataframes['Dataset Name']")

# === CELL 1: SETUP AND DATA LOADING ===

import pandas as pd
import numpy as np
import json
import os

# ML imports
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_score, recall_score, f1_score

import xgboost as xgb
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns

print("Loading dataset from Kaggle cache...")

# Dataset is already in this path
# The dataset was downloaded by the first cell to: /root/.cache/kagglehub/datasets/computingvictor/transactions-fraud-datasets/versions/1
dataset_path = '/root/.cache/kagglehub/datasets/computingvictor/transactions-fraud-datasets/versions/1'

# Load CSV files
transaction_df = pd.read_csv(os.path.join(dataset_path, "transactions_data.csv"))
card_df = pd.read_csv(os.path.join(dataset_path, "cards_data.csv"))
users_df = pd.read_csv(os.path.join(dataset_path, "users_data.csv"))

# Load JSON files
with open(os.path.join(dataset_path, "train_fraud_labels.json"), 'r') as f:
    raw_json_data = json.load(f)
transaction_labels_dict = raw_json_data['target']
train_fraud_labels = pd.Series(transaction_labels_dict).reset_index()
train_fraud_labels.columns = ['transaction_id', 'is_fraud']
train_fraud_labels['transaction_id'] = pd.to_numeric(train_fraud_labels['transaction_id'])

with open(os.path.join(dataset_path, "mcc_codes.json"), 'r') as f:
    mcc_series_data = json.load(f)
mcc_series = pd.Series(mcc_series_data)
mcc_df = mcc_series.reset_index()
mcc_df.columns = ['mcc_code', 'description']

print("All data files loaded successfully!")
print(f"Transactions: {transaction_df.shape}")
print(f"Cards: {card_df.shape}")
print(f"Users: {users_df.shape}")
print(f"Fraud Labels: {train_fraud_labels.shape}")
print(f"MCC Codes: {mcc_df.shape}")

# Work with data sample to avoid RAM issues
sample_size = 1000000
df_sample = transaction_df.sample(n=sample_size, random_state=42)

# Convert mcc column to string for merging
df_sample['mcc'] = df_sample['mcc'].astype(str)
mcc_df['mcc_code'] = mcc_df['mcc_code'].astype(str)

# Merge sample with other tables
df = pd.merge(df_sample, train_fraud_labels, left_on='id', right_on='transaction_id', how='left')
df = pd.merge(df, card_df, left_on='card_id', right_on='id', how='left', suffixes=('', '_card'))
df = pd.merge(df, users_df, left_on='client_id', right_on='id', how='left', suffixes=('', '_user'))
df = pd.merge(df, mcc_df, left_on='mcc', right_on='mcc_code', how='left')

# Clean up redundant columns
df = df.drop(columns=['transaction_id', 'id_card', 'id_user', 'mcc_code'])

print(f"Sample DataFrame shape: {df.shape}")

# Remove unlabeled transactions and convert target to numeric
df.dropna(subset=['is_fraud'], inplace=True)
df['is_fraud'] = df['is_fraud'].map({'No': 0, 'Yes': 1})

# Split into features and target
features = [col for col in df.columns if col != 'is_fraud']
X = df[features]
y = df['is_fraud']

from sklearn.model_selection import train_test_split
import gc

# Recreate X and y from the sampled dataframe
features = [col for col in df.columns if col != 'is_fraud']
X = df[features]
y = df['is_fraud']

# Split data into train (60%), validation (20%), and test (20%)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.40, random_state=42, stratify=y)
X_cv, X_test, y_cv, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp)

# Free memory by deleting intermediate DataFrames
del X, y, X_temp, y_temp
gc.collect()

print("Data split completed:")
print(f"Training: {X_train.shape}")
print(f"Validation: {X_cv.shape}")
print(f"Test: {X_test.shape}")

import numpy as np
import pandas as pd

def apply_preprocessing(df, is_training_set=False, median_imputations=None):
    """Apply feature engineering and preprocessing steps."""
    df_processed = df.copy()

    # Clean numerical columns by removing $ and commas
    amount_cols = ['amount', 'per_capita_income', 'yearly_income', 'credit_limit', 'total_debt']
    for col in amount_cols:
        if col in df_processed.columns:
            df_processed[col] = pd.to_numeric(df_processed[col].astype(str).str.replace(r'[$,]', '', regex=True), errors='coerce')

    # Extract features from date columns
    date_cols = ['date', 'expires', 'acct_open_date']
    for col in date_cols:
        if col in df_processed.columns:
            df_processed[col] = pd.to_datetime(df_processed[col], errors='coerce', format='mixed')

    if 'date' in df_processed.columns:
        df_processed['hour_of_day'] = df_processed['date'].dt.hour
        df_processed['day_of_week'] = df_processed['date'].dt.dayofweek
        df_processed['month'] = df_processed['date'].dt.month
    if 'expires' in df_processed.columns and 'date' in df_processed.columns:
        df_processed['days_to_expiry'] = (df_processed['expires'] - df_processed['date']).dt.days

    df_processed.drop(columns=date_cols, inplace=True)

    # Create cyclical features for time-based columns
    cyclical_cols = ['hour_of_day', 'day_of_week', 'month']
    if all(col in df_processed.columns for col in cyclical_cols):
        df_processed['hour_sin'] = np.sin(2 * np.pi * df_processed['hour_of_day'] / 24.0)
        df_processed['hour_cos'] = np.cos(2 * np.pi * df_processed['hour_of_day'] / 24.0)
        df_processed['day_of_week_sin'] = np.sin(2 * np.pi * df_processed['day_of_week'] / 7.0)
        df_processed['day_of_week_cos'] = np.cos(2 * np.pi * df_processed['day_of_week'] / 7.0)
        df_processed['month_sin'] = np.sin(2 * np.pi * df_processed['month'] / 12.0)
        df_processed['month_cos'] = np.cos(2 * np.pi * df_processed['month'] / 12.0)
        df_processed.drop(columns=cyclical_cols, inplace=True)

    # Process binary categorical features
    if 'errors' in df_processed.columns:
        df_processed['has_error'] = df_processed['errors'].notna().astype(int)
    if 'gender' in df_processed.columns:
        df_processed['gender'] = df_processed['gender'].map({'Female': 0, 'Male': 1})
    if 'has_chip' in df_processed.columns:
        df_processed['has_chip'] = df_processed['has_chip'].map({'NO': 0, 'YES': 1})

    # Handle missing values with median imputation
    numerical_cols = df_processed.select_dtypes(include=np.number).columns.tolist()
    if is_training_set:
        median_imputations = df_processed[numerical_cols].median()

    if median_imputations is not None:
        df_processed.fillna(median_imputations, inplace=True)

    return df_processed, median_imputations

print("Preprocessing function defined.")

# Apply feature engineering to all data splits
print("Applying feature engineering to all data splits...")

# Process training set and learn medians for imputation
X_train, median_imputations_dict = apply_preprocessing(X_train, is_training_set=True)

# Apply same preprocessing to validation and test sets using training medians
X_cv, _ = apply_preprocessing(X_cv, median_imputations=median_imputations_dict)
X_test, _ = apply_preprocessing(X_test, median_imputations=median_imputations_dict)
gc.collect()

print('Feature engineering completed.')
print(f"X_train shape: {X_train.shape}")
print(f"X_train columns (first 10): {X_train.columns.tolist()[:10]}...")

# Drop unnecessary columns from all datasets
print("Dropping unnecessary columns...")

cols_to_drop = [
    'id', 'client_id', 'card_id', 'merchant_id', 'card_number', 'cvv', 'mcc',
    'acct_open_date', 'year_pin_last_changed', 'card_on_dark_web', 'has_chip',
    'address', 'merchant_city', 'birth_year', 'birth_month', 'latitude',
    'longitude', 'date', 'expires'
]

for dataset in [X_train, X_cv, X_test]:
    existing_cols = [col for col in cols_to_drop if col in dataset.columns]
    dataset.drop(columns=existing_cols, inplace=True, errors='ignore')

print('Unnecessary columns dropped.')
print(f"X_train shape: {X_train.shape}")
print(f"X_train columns (first 10): {X_train.columns.tolist()[:10]}...")

from sklearn.preprocessing import OneHotEncoder

print("Grouping and one-hot encoding categorical features...")

# Group merchant_state based on training set fraud patterns
temp_train = pd.DataFrame({'merchant_state': X_train['merchant_state'], 'is_fraud': y_train})
fraud_counts = temp_train[temp_train['is_fraud'] == 1]['merchant_state'].value_counts()
top_15_fraud_states = fraud_counts.nlargest(15).index.tolist()
del temp_train, fraud_counts

# Apply state grouping to all datasets
for dataset in [X_train, X_cv, X_test]:
    if 'merchant_state' in dataset.columns:
        dataset.loc[:, 'merchant_state'] = dataset['merchant_state'].apply(
            lambda x: x if x in top_15_fraud_states else 'OTHER_STATE'
        )

# One-hot encode all remaining categorical columns
categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()
print(f'Categorical columns to encode: {categorical_cols}')

encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.int8)
encoder.fit(X_train[categorical_cols])

# Transform all datasets
encoded_train = pd.DataFrame(encoder.transform(X_train[categorical_cols]), index=X_train.index, columns=encoder.get_feature_names_out(categorical_cols))
encoded_cv = pd.DataFrame(encoder.transform(X_cv[categorical_cols]), index=X_cv.index, columns=encoder.get_feature_names_out(categorical_cols))
encoded_test = pd.DataFrame(encoder.transform(X_test[categorical_cols]), index=X_test.index, columns=encoder.get_feature_names_out(categorical_cols))

X_train = pd.concat([X_train.drop(columns=categorical_cols), encoded_train], axis=1)
X_cv = pd.concat([X_cv.drop(columns=categorical_cols), encoded_cv], axis=1)
X_test = pd.concat([X_test.drop(columns=categorical_cols), encoded_test], axis=1)
gc.collect()

print('One-hot encoding completed.')
print(f"X_train shape: {X_train.shape}")
print(f"X_train columns (first 10): {X_train.columns.tolist()[:10]}...")

print("Downcasting data types for memory efficiency...")
for dataset in [X_train, X_cv, X_test]:
    for col in dataset.select_dtypes(include=['float64', 'int64']).columns:
        if 'float' in str(dataset[col].dtype):
            dataset.loc[:, col] = dataset[col].astype('float32')
        else:
            dataset.loc[:, col] = pd.to_numeric(dataset[col], downcast='integer')
gc.collect()

print("\nPreprocessing completed.")
print(f"X_train shape: {X_train.shape}")
print(f"X_cv shape: {X_cv.shape}")
print(f"X_test shape: {X_test.shape}")

print("Final NaN check across all datasets...")

nan_train = X_train.isna().sum()
nan_cv = X_cv.isna().sum()
nan_test = X_test.isna().sum()

print(f"X_train NaNs: {nan_train[nan_train > 0]}")
print(f"X_cv NaNs: {nan_cv[nan_cv > 0]}")
print(f"X_test NaNs: {nan_test[nan_test > 0]}")

if nan_train[nan_train > 0].empty and nan_cv[nan_cv > 0].empty and nan_test[nan_test > 0].empty:
    print("All datasets are clean - ready for modeling!")
else:
    print("WARNING: NaNs still present in datasets.")

# Calculate scale_pos_weight for handling class imbalance
neg_count = y_train.value_counts()[0]
pos_count = y_train.value_counts()[1]
scale_pos_weight_value = neg_count / pos_count

print(f"Legitimate transactions: {neg_count}")
print(f"Fraudulent transactions: {pos_count}")
print(f"scale_pos_weight: {scale_pos_weight_value:.2f}")

import xgboost as xgb

# Initialize and train XGBoost model
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    scale_pos_weight=scale_pos_weight_value,
    random_state=42,
    n_estimators=500,
    learning_rate=0.05,
    max_depth=5,
    subsample=0.7,
    colsample_bytree=0.7,
    gamma=0.1
)

print("Training XGBoost model...")
xgb_model.fit(X_train, y_train)
print("Model training complete.\n")

import joblib

SAVE_PATH = "fraud_detection_pipeline.pkl"

joblib.dump(
    {
        "model": xgb_model,
        "encoder": encoder,
        "median_imputations": median_imputations_dict
    },
    SAVE_PATH
)

print(f"âœ… Model saved successfully as: {SAVE_PATH}")

from google.colab import files
files.download("fraud_detection_pipeline.pkl")



# Evaluate on validation set
y_pred_cv = xgb_model.predict(X_cv)
y_proba_cv = xgb_model.predict_proba(X_cv)[:, 1]

print("Validation Set Evaluation:")
print(f"Confusion Matrix:\n{confusion_matrix(y_cv, y_pred_cv)}")
print(f"Classification Report:\n{classification_report(y_cv, y_pred_cv)}")
print(f"ROC AUC Score: {roc_auc_score(y_cv, y_proba_cv):.4f}")

# Show top feature importances
print("\nTop 15 Feature Importances:")
feature_importances = pd.Series(xgb_model.feature_importances_, index=X_train.columns)
print(feature_importances.nlargest(15))

# Test the model on unseen data
y_pred_test = xgb_model.predict(X_test)
y_proba_test = xgb_model.predict_proba(X_test)[:, 1]

print("Test Set Performance:")
print(f"ROC AUC: {roc_auc_score(y_test, y_proba_test):.4f}")
print(classification_report(y_test, y_pred_test))

